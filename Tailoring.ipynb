{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82dba7-8f8d-4e36-9d53-ba65d5fa0518",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "741505b8-202c-44da-b7b9-ab894f2349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import docx\n",
    "from docx import Document\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import openai\n",
    "openai.api_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "481c7fd8-0ff5-470d-8305-8c72ee2b92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_path = 'CV Rodrigo Ugarte.docx'\n",
    "cover_letter_path = 'Cover Letter.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14b2f1-2aeb-4ba5-a65d-1d088cc41504",
   "metadata": {},
   "source": [
    "# CV TAILORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6dcbf-0bde-4bda-93fe-c0d6bb90fa63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First tries\n",
    "I want to keep ths function here, as they were my first attempts to try and replace the action verbs of my cv with action verbs with the job offer. I had some success but dealing with verb conjugation was really tough so I decided it was more effective to treat the bullet points of the cv with open Ai gpt-4o, as its not much generated text the extra incurring costs are not even 1c. However, going forward it would be interesting to complement and turn this functions into my original idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7bb8ab5-6018-4726-ab24-71156f65499a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from docx import Document\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def extract_job_info(file_path):\n",
    "    # Extract text from the document\n",
    "    job_offer_text = extract_text(file_path)\n",
    "    \n",
    "    # Preprocess text\n",
    "    job_offer_text = job_offer_text.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # Apply NLP model\n",
    "    doc = nlp(job_offer_text)\n",
    "    \n",
    "    # Custom matcher patterns for skills\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    skill_patterns = [\n",
    "        [{\"LOWER\": \"python\"}],\n",
    "        [{\"LOWER\": \"machine learning\"}],\n",
    "        [{\"LOWER\": \"data analysis\"}],\n",
    "        [{\"LOWER\": \"project management\"}],\n",
    "        [{\"LOWER\": \"sql\"}],\n",
    "        [{\"LOWER\": \"excel\"}]\n",
    "        # Add more patterns as needed\n",
    "    ]\n",
    "\n",
    "    # Add patterns to the matcher\n",
    "    for pattern in skill_patterns:\n",
    "        matcher.add(\"SKILL\", [pattern])\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Extract entities using SpaCy's built-in NER\n",
    "    skills = set()\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills.add(span.text)\n",
    "    \n",
    "    values = set([ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"NORP\", \"FAC\", \"EVENT\", \"LAW\", \"LOC\", \"PRODUCT\", \"WORK_OF_ART\", \"LANGUAGE\"]])\n",
    "    initiatives = set([ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"NORP\", \"FAC\", \"EVENT\", \"LAW\", \"LOC\", \"PRODUCT\", \"WORK_OF_ART\", \"LANGUAGE\"]])\n",
    "\n",
    "    # Extract action verbs\n",
    "    action_verbs = set([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "    return {\n",
    "        \"skills\": list(skills),\n",
    "        \"values\": list(values),\n",
    "        \"initiatives\": list(initiatives),\n",
    "        \"action_verbs\": list(action_verbs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d50e35e5-bc49-476c-957f-3744ce929a1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rodri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rodri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "# action verbs exclusive from my CV to be replaced\n",
    "my_action_verbs = {\n",
    "    \"automated\", \"implemented\", \"developed\", \"created\", \"directed\", \"achieved\",\n",
    "    \"led\", \"deployed\", \"enhances\", \"engaged\", \"integrated\", \"interpreted\", \n",
    "    \"built\", \"leveraged\", \"formulated\"}\n",
    "\n",
    "# WordNet for synonims\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return synonyms\n",
    "\n",
    "# extract verbs from text using spaCy\n",
    "def extract_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    verbs = set([token.text for token in doc if token.pos_ == 'VERB'])\n",
    "    return verbs\n",
    "\n",
    "# replace verbs in the document\n",
    "def replace_verbs(doc, replacements):\n",
    "    for paragraph in doc.paragraphs:\n",
    "        for key, value in replacements.items():\n",
    "            if key in paragraph.text:\n",
    "                doc_nlp = nlp(paragraph.text)\n",
    "                new_text = []\n",
    "                for token in doc_nlp:\n",
    "                    if token.text.lower() == key:\n",
    "                        new_text.append(conjugate_verb(token.text, value))\n",
    "                    else:\n",
    "                        new_text.append(token.text)\n",
    "                paragraph.text = \" \".join(new_text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7684592-ad32-4a60-a1ce-e8d5acf61140",
   "metadata": {},
   "source": [
    "## CV Tailoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4f287fcc-d029-4f4a-8de3-537d677d98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cv(cv_path):\n",
    "    doc = docx.Document(cv_path)\n",
    "    cv_text = {'experience': [], 'projects': []}\n",
    "    current_section = None\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "\n",
    "        if \"experience\" in text.lower():\n",
    "            current_section = 'experience'\n",
    "        elif \"relevant projects\" in text.lower():\n",
    "            current_section = 'projects'\n",
    "        elif current_section == 'experience':\n",
    "            if text and not text[0].isdigit() and not text.lower().startswith(('logistic', 'atria corp', 'luz del sur', 'operations and data intern', 'falcon management partners', 'consulting analyst')):\n",
    "                cv_text['experience'].append(text)\n",
    "        elif current_section == 'projects':\n",
    "            if text and not text.lower().startswith(('development of machine learning algorithms', 'cv and cover letter tailoring', 'spotify:', 'big foot sightings')):\n",
    "                cv_text['projects'].append(text)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "def update_cv_sections(cv_text, job_description):\n",
    "    # Prepare the prompt\n",
    "    prompt_text = (f\"Given the job description below, update the CV bullet points accordingly. Ensure you produce the same number (17) of bullet points as inputed\"\n",
    "                   f\"Maintain the ideas and lengths (LESS THAN 15 WORDS) of the bullet points, but use the ACTION VERBS AND KEY WORDS FROM THE JOB OFFER\"\n",
    "                   f\"Do not make drastic changes and do not repeat the same action verb more than twice.\\n\\n\"\n",
    "                   f\"Job Description: {job_description}\\n\\n\"\n",
    "                   f\"CV Experience Section: {' '.join(cv_text['experience'])}\\n\\n\"\n",
    "                   f\"CV Projects Section: {' '.join(cv_text['projects'])}\")\n",
    "\n",
    "    # API call\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a recruiter with 20 years of experience in big tech companies, expert in CV tailoring.\"}, \n",
    "                  {\"role\": \"user\", \"content\": prompt_text}],\n",
    "        max_tokens=450)\n",
    "\n",
    "    new_bullet_points = response.choices[0].message.content\n",
    "    return new_bullet_points\n",
    "\n",
    "def update_cv(cv_path, new_bullets_text):\n",
    "    # extract new bullet points from the generated text\n",
    "    new_bullets = [line.strip() for line in new_bullets_text.split('\\n') if line.strip() and not line.startswith(\"###\")]\n",
    "\n",
    "    # format out the titles\n",
    "    if len(new_bullets) > 17:\n",
    "        bullets1 = new_bullets[1:10]\n",
    "        bullets2 = new_bullets[11:20]\n",
    "        new_bullets = bullets1 + bullets2\n",
    "\n",
    "    # extract current bullet points using read_cv\n",
    "    current_cv = read_cv(cv_path)\n",
    "    current_bullets = current_cv['experience'] + current_cv['projects']\n",
    "\n",
    "    #if len(new_bullets) != 17 or len(current_bullets) != 17:\n",
    "        #raise ValueError(f\"Either the number of new bullet points ({len(new_bullets)}) or the current bullet points ({len(current_bullets)}) is not 17\")\n",
    "\n",
    "    # using DOCX file\n",
    "    doc = docx.Document(cv_path)\n",
    "    bullet_idx = 0\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if text in current_bullets:\n",
    "            # Copy the existing formatting\n",
    "            run = para.runs[0]\n",
    "            new_text = new_bullets[bullet_idx][2:]\n",
    "            \n",
    "            # Clear the current text and apply new text with the same formatting\n",
    "            para.clear()\n",
    "            new_run = para.add_run(new_text)\n",
    "            new_run.bold = run.bold\n",
    "            new_run.italic = run.italic\n",
    "            new_run.underline = run.underline\n",
    "            new_run.font.size = run.font.size\n",
    "            new_run.font.name = run.font.name\n",
    "            new_run.font.color.rgb = run.font.color.rgb\n",
    "            \n",
    "            bullet_idx += 1\n",
    "\n",
    "    return doc\n",
    "    \n",
    "def generate_cv(cv_path, job_offer):\n",
    "    cv_text = read_cv(cv_path)\n",
    "    new_bullets_text = update_cv_sections(cv_text, job_offer)\n",
    "    return update_cv(cv_path, new_bullets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "48d40995-8c4b-4263-ac19-17eeb20a3fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updated_CV_Rodrigo_Ugarte.docx'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_cv(cv_path, new_bullets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed63e5d-b857-4461-af29-5a1ec43f4ddd",
   "metadata": {},
   "source": [
    "# COVER LETTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba698f-753f-4eb6-acb1-8152f4874d7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## APPROACH 1 (SECTION GENERATION AND THEN REVIEW BY OPEN AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b433acc7-1c3f-4ecd-bd05-90158952dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text_in_docx(filename, company, position):\n",
    "    doc = Document(filename)\n",
    "    replacements = {\n",
    "    '[COMPANY]': company,\n",
    "    '[POSITION]': position}\n",
    "    for para in doc.paragraphs:\n",
    "        for key, value in replacements.items():\n",
    "            if key in para.text:\n",
    "                para.text = para.text.replace(key, value)\n",
    "    return doc\n",
    "    \n",
    "def extract_paragraph(filename):\n",
    "    doc = Document(filename)\n",
    "    target_heading = \"Alignment with Values:\"\n",
    "    is_next_paragraph = False\n",
    "    for para in doc.paragraphs:\n",
    "        if is_next_paragraph:\n",
    "            values_paragraph = para.text\n",
    "            return values_paragraph\n",
    "        if target_heading in para.text:\n",
    "            is_next_paragraph = True\n",
    "            \n",
    "def format_paragraph_with_gpt4o(cover_letter_path, job_offer):\n",
    "\n",
    "    values_paragraph = extract_paragraph(cover_letter_path)\n",
    "    \n",
    "    prompt_text = (\n",
    "        \"You are a recruiter with 20 years of experience in big tech companies. \"\n",
    "        \"Based on the detailed job information provided below, craft a concise and compelling paragraph with around 150-200 words for a cover letter. \"\n",
    "        \"This paragraph should articulate why I am drawn to the company, specifically citing relevant company values \"\n",
    "        \"and initiatives that you can find mainly online or mentioned in the job offer. Please align this with the professional writing style \"\n",
    "        \"outlined in the provided values paragraph. Ensure the response is tailored to reflect the unique aspects \"\n",
    "        \"of both the job information and the values paragraph.\\n\\n\"\n",
    "        f\"Job Information: {job_offer}\\n\\n\"\n",
    "        f\"Values Paragraph: {values_paragraph}\")\n",
    "    \n",
    "    # API call\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You are a recruiter with 20 years of experience in big tech companies, expert in CV and cover letter taloring\"}, \n",
    "                  {\"role\": \"user\", \"content\": prompt_text}],\n",
    "        max_tokens=250)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def replace_para_in_docx(filename, new_text):\n",
    "    doc = Document(filename)\n",
    "    target_heading = \"Alignment with Values:\"\n",
    "    is_next_paragraph = False\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        if is_next_paragraph:\n",
    "            para.text = new_text\n",
    "            break\n",
    "        if target_heading in para.text:\n",
    "            is_next_paragraph = True\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d322d8f6-c306-41bc-8cf9-595bbe7ad762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cover_letter(job_offer):\n",
    "\n",
    "    # cover letter text\n",
    "    cover_letter_path = 'Cover Letter.docx'\n",
    "    cover_letter_template = extract_text(cover_letter_path)\n",
    "    \n",
    "    # prompt for GPT-4o\n",
    "    prompt_text = (f\"You are a recruiter with 20 years of experience in big tech companies. \"\n",
    "               f\"Using the cover letter below and the Job Offer Details, review the cover letter and leverage your expertise to make pertinent changes. \"\n",
    "               f\"Do not make any drastic changes and always match the tone and writing style to keep it the same. \"\n",
    "               f\"The cover letter should articulate why the applicant is drawn to the company and showcase their skills, experiences, and competencies, \"\n",
    "               f\"specifically citing relevant company values and initiatives mentioned in the job offer, but mainly those found online. \"\n",
    "               f\"Keep it under 470 words, so trim information that is not relevant to the job offer and make sure you return a cover letter from a person you would definitely hire.\\n\\n\"\n",
    "               f\"Job Offer Details: {job_offer}\\n\\n\"\n",
    "               f\"Cover Letter Template: {cover_letter_template}\")\n",
    "\n",
    "    # API call\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You are a recruiter with 20 years of experience in big tech companies, expert in CV and cover letter taloring\"}, \n",
    "                  {\"role\": \"user\", \"content\": prompt_text}],\n",
    "        max_tokens=600)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "09b2c621-c1f4-4491-8f92-ba5fc3d672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step_tailor():\n",
    "    job_df = pd.read_csv('Cover Letter List.csv', encoding='latin-1')\n",
    "    for index, row in job_df.iterrows():\n",
    "        company = row['Company']\n",
    "        position = row['Position']\n",
    "        job_offer = row['Job Offer']\n",
    "        replace_text_in_docx(cover_letter_path, company, position)\n",
    "        new_para = format_paragraph_with_gpt4o(cover_letter_path, job_offer)\n",
    "        cover_letter_new_para = replace_para_in_docx(cover_letter_path, new_para)\n",
    "        cover_letter_text = review_cover_letter(job_offer)\n",
    "        \n",
    "        # save the generated cover letter to a file\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(cover_letter_text)\n",
    "        doc.save(f'1STEP{company}_{position}_Cover_Letter.docx')\n",
    "        print(f'Cover Letter for {company} for {position} done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "076b2e11-ce4f-40b5-8bee-ef20c2b61223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Letter for Facebook for Data Engineer done!\n",
      "Cover Letter for Hazen Research for Entry Level Data Scientist done!\n",
      "Cover Letter for Facebook2 for Data Engineer done!\n",
      "Cover Letter for Hazen Research2 for Entry Level Data Scientist done!\n"
     ]
    }
   ],
   "source": [
    "two_step_tailor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56ab8-251a-4077-bdda-8bbba52e9275",
   "metadata": {},
   "source": [
    "## APPROACH 2 (ALL GENERATION BY OPEN AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8b245526-2e37-445e-aee9-41e749b7e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(company, position, job_offer):\n",
    "\n",
    "    cover_letter_path = 'Cover Letter.docx'\n",
    "    # cover letter text\n",
    "    cover_letter_template = extract_text(cover_letter_path)\n",
    "    \n",
    "    #job_offer_text = extract_text(job_offer_path)\n",
    "    \n",
    "    # prompt for GPT-4o\n",
    "    prompt_text = (f\"Using the cover letter template and Job Offer Details provided below, craft a complete, compelling cover letter. \"\n",
    "                   f\"The cover letter should articulate why the applicant is drawn to {company}, and fit to be a {position}and showcase my skills, experiences and competences \"\n",
    "                   f\"Leverage all my qualities, also showing my softskills, and dont invent false information \" \n",
    "                   f\"specifically citing relevant company values and initiatives mentioned in the job offer but mainly ones found online. \"\n",
    "                   f\"Keep it under 470 words so trim information on the template that is not relevant to the job offer and make sure you return a cover letter from a person you would definitely hire.\"\n",
    "                   f\"Maintain a professional writing style and match the writing style of the template\"\n",
    "                   f\"Moreover, I am attacching bullet points from my cv, please adapt them with the info of the cover letter and my cover letter\\n\\n\"\n",
    "                   f\"Job Offer Details: {job_offer}\\n\\n\"\n",
    "                   f\"Cover Letter Template: {cover_letter_template}\")\n",
    "    \n",
    "    # API call\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You are a recruiter with 20 years of experience in big tech companies, expert in CV and cover letter taloring\"}, \n",
    "                  {\"role\": \"user\", \"content\": prompt_text}],\n",
    "        max_tokens=620)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7a3fe346-2319-402f-9ff3-c070c8702cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_tailor():\n",
    "    job_df = pd.read_csv('Cover Letter List.csv', encoding='latin-1')\n",
    "    for index, row in job_df.iterrows():\n",
    "        company = row['Company']\n",
    "        position = row['Position']\n",
    "        job_offer = row['Job Offer']\n",
    "        cover_letter = generate_cover_letter(company, position, job_offer)\n",
    "        \n",
    "        # save the generated cover letter to a file\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(cover_letter)\n",
    "        doc.save(f'2STEP_Cover_Letter_{company}_{position}.docx')\n",
    "        print(f'Cover Letter for {company} for {position} done!')\n",
    "        \n",
    "        # save CV to a file\n",
    "        #cv_path = 'CV Rodrigo Ugarte.docx'\n",
    "        #cv = generate_cv(cv_path, job_offer)\n",
    "        #cv.save(f'CV_Rodrigo_Ugarte_{company}_{position}.docx')\n",
    "        #print(f'CV for {company} for {position} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d7c5ee29-4020-414e-a47e-3a34028232cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Letter for Facebook for Data Engineer done!\n",
      "Cover Letter for Hazen Research for Entry Level Data Scientist done!\n",
      "Cover Letter for Facebook2 for Data Engineer done!\n",
      "Cover Letter for Hazen Research2 for Entry Level Data Scientist done!\n"
     ]
    }
   ],
   "source": [
    "one_step_tailor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26275d6-7e44-4253-9b1f-204bc6d4bdfe",
   "metadata": {},
   "source": [
    "### 1 Step generation = Faster!\n",
    "**Total Cost**: 0.55 --> 0.62 ; Delta=0.07\n",
    "<br>**Full Job Offer**\n",
    "<br>Price: **0.02**\n",
    "<br>Quality: 5/5\n",
    "<br>**Summarized Job Offer**\n",
    "<br>Price: **0.015**\n",
    "<br>Quality: 3.5/5\n",
    "\n",
    "### 2 Step generation\n",
    "**Total Cost**: 0.45 --> 0.55 ; Delta=0.10\n",
    "<br>**Full Job Offer**:\n",
    "<br>Price: **0.03**\n",
    "<br>Quality: 4.5/5\n",
    "<br>**Summarized Job Offer**\n",
    "<br>Price: **0.02**\n",
    "<br>Quality: 3/5\n",
    "\n",
    "# Winner: 1 Step Generation with Full Offer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a3736-01e8-4c6c-9566-b5742eea7a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
